{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/shamil-k/Salary-And-Satisfaction-Prediction-Using_RandomForest/blob/main/final.ipynb",
      "authorship_tag": "ABX9TyMq4n6ycz+fTMFGEjeCTjkt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shamil-k/Salary-And-Satisfaction-Prediction-Using_RandomForest/blob/main/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiWvZ8MMB5g6"
      },
      "source": [
        "#  Business Understanding\n",
        "\n",
        "**Problem Statment**\n",
        "\n",
        "You have to clean this data, prepare it for ML models, build more features and ultimately build a random forest (scikit).\n",
        "Split the dataset into 80:20, train:test. \n",
        "Print the Weighted F1 on test dataset (target_satisfaction) and RMSE (Target_salary)\n",
        "\n",
        "\n",
        "**My Observation**\n",
        "\n",
        "Our Requirement to build a RandomForest Model for this data set. In this \n",
        "record we have less amount of record.i will use clustering/ANN ... if only we have more number of data.From my experience Supervised Machine Learning is not a good approach for real world scenarios Because in this world containing 5:95 Labelled and Unlabelled .It is the Big challenge to pattern and making understand with out human help. it will already found the solution from Various AI research and deployment platforms \n",
        "Eg:  GPT3, BERT\n",
        "\n",
        "Here we are using Ensamble Technique in this we will get better result boosting instead of bagging we only can tune the Hyper parameter from this because it is just  prediction of multiple tree based algorith technically Bootsrap Aggression.\n",
        "\n",
        "\n",
        "**Result**\n",
        "\n",
        "I approch following step to get the better result  Random Forest In Classification  Accuracy : 94% , Regression (R^2) Accuracy: 100% \n",
        "\n",
        "We will get more Better By adding more records so that we can fine tuning and we can build our own model that it defining the weight from our own datas instead of using pretrained by using the algorithms \n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "Please let me know if you’d be open to a conversation to discuss this position. I’m happy to provide you with any additional information you might need. I look forward to hearing from you.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cC6W6OxBSDU"
      },
      "source": [
        "Steps i followed:\n",
        "\n",
        "* data mining-extract the data\n",
        "\n",
        "* data cleaning-Filled Null values\n",
        "                Removed Unused  Characters\n",
        "                Encoding categorical features with Different encoding methods\n",
        "                (Target Encoding, Target Guided Ordinal Encoding,BinaryEncoder)       \n",
        "                Balanced the data set with OverSampling\n",
        "* Modeling - Create Model with tuning hyper parameter with GridSearchCV\n",
        "             Splitted the data (80:20)\n",
        "* Evaluation - Evaluating the model\n",
        "             Checking the Co-Realtions ..\n",
        "* Deploy   -  \n",
        "            Save Model After Evaluating \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Used Features After Analysing and Evaluation**\n",
        "\n",
        "LOAN_AMT,\tBusiness Title,\tCivil Service Title,\tDivision/Work Unit,\tFormalEducation,\tUndergradMajor,\tCompanySize,\n",
        "DevType,\tYearsCodingProf,\tHopeFiveYears,\tJobSearchStatus,\tLastNewJob,\tAgreeDisagree1,\tAge,\tterm,\tloan_status,\n",
        "purpose,\ttitle,\tdti,\tearliest_cr_line,\tlast_pymnt_d, last_credit_pull_d,\temp_length, dateAdded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhRxE8okT_dT"
      },
      "source": [
        "# some of my references:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfR7-p0BRbC4"
      },
      "source": [
        "\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
        "\n",
        "https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf\n",
        "\n",
        "https://github.com/shamil-k/Feature-Engineer-All-You-need\n",
        "\n",
        "\n",
        "https://towardsdatascience.com/getting-deeper-into-categorical-encodings-for-machine-learning-2312acd347c8\n",
        "\n",
        "https://github.com/shamil-k/Feature-Engineer-All-You-need\n",
        "                https://heartbeat.fritz.ai/\n",
        "              \n",
        "\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/\n",
        "               \n",
        "               \n",
        "https://contrib.scikit-learn.org/category_encoders/hashing.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1gXCV33UJQf"
      },
      "source": [
        "# Data Mining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buwdtfXvntXR"
      },
      "source": [
        "# importing necessary libraries \n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "!pip install category_encoders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ehZPo6zDjsk"
      },
      "source": [
        "\n",
        "**Reading The Data Set Using Pandas.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLGlG5ktb3aR"
      },
      "source": [
        "# Reading and converting into dataframe\n",
        "df = pd.read_csv('/content/drive/MyDrive/New folder/salary,satisfaction - salary,satisfaction.csv', parse_dates=True, dayfirst=True, index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UgLSnKND3-0"
      },
      "source": [
        "# Data Cleaning/Preprocessing  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aztw2UsLb70N"
      },
      "source": [
        "# Checking the features having null values\n",
        "df.isnull().sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu4ug79_VSdf"
      },
      "source": [
        "# checking the features data type\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfyWJWiFcNYS"
      },
      "source": [
        "# Filling the null values\n",
        "df['FormalEducation'] = df['FormalEducation'].fillna('Not-Specified')\n",
        "df['UndergradMajor'] = df['UndergradMajor'].fillna('Not-Specified')\n",
        "df['DevType'] = df['DevType'].fillna('Not-Specified')\n",
        "df['HopeFiveYears'] = df['HopeFiveYears'].fillna('Not-Specified')\n",
        "df['YearsCoding'] = df['YearsCoding'].fillna('0-2')\n",
        "df['YearsCodingProf'] = df['YearsCodingProf'].fillna('0-2')\n",
        "df['JobSearchStatus'] = df['JobSearchStatus'].fillna('Not-Specified')\n",
        "df['LastNewJob'] = df['LastNewJob'].fillna('Not-Specified')\n",
        "df['EducationTypes'] = df['EducationTypes'].fillna('Not-Specified')\n",
        "df['AgreeDisagree1'] = df['AgreeDisagree1'].fillna('Not-Specified')\n",
        "df['Age'] = df['Age'].fillna('Not-Specified')\n",
        "df['emp_length'] = df['emp_length'].fillna('< 1 year')\n",
        "df['Age'] = df['Age'].fillna('Not-Specified')\n",
        "df['title'] = df['title'].fillna('Not-Specified')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdOQX3fm0qIS"
      },
      "source": [
        "#removing the unnecessary unique characters \n",
        "chars_to_remove = ['years', 'or more years', 'years old', '$', '<']\n",
        "regular_expression = '[' + re.escape (''. join (chars_to_remove)) + ']'\n",
        "df['YearsCodingProf'] = df['YearsCodingProf'].str.replace(regular_expression, '', regex=True) \n",
        "df['title'] = df['title'].str.replace(regular_expression, '', regex = True)\n",
        "df['LOAN_AMT'] = df['LOAN_AMT'].str.replace(regular_expression, '', regex = True)\n",
        "df['emp_length'] = df['emp_length'].str.replace(regular_expression, '', regex = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCZGz84fXoaC"
      },
      "source": [
        "# Checking weather the null values filled or not\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VZvEtcu00JC"
      },
      "source": [
        "# Reset indexing and here dropping some ( if Same/Unique/Making High Error) features\n",
        "df  = df.reset_index()\n",
        "df = df.drop(['id', 'Time', 'EducationTypes',  'latitude', 'longitude', 'YearsCoding' ], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_z9PXmHFrdj"
      },
      "source": [
        "## Encoding Categorical Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngtw50F26i0D"
      },
      "source": [
        "# importing the library for encoding\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import category_encoders as ce"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZFrfDH03WRv"
      },
      "source": [
        "# checking the labels from features\n",
        "df.apply(lambda x: len(x.unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KTzsO-bbSmI"
      },
      "source": [
        "# checking how many catogorical featrues are there\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnqRGke18w7j"
      },
      "source": [
        "# Encoding Using TargetEncoder from category_encoder\n",
        "# it will never increasing the dimensions it groupby with target variable and putting the value\n",
        "encoder = ce.TargetEncoder(cols=['Business Title'])\n",
        "encoder\n",
        "df['Business Title'] = encoder.fit_transform(df['Business Title'],df['Target_Salary'])\n",
        "encoder = ce.TargetEncoder(cols=['FormalEducation'])\n",
        "encoder\n",
        "df['FormalEducation'] = encoder.fit_transform(df['FormalEducation'],df['Target_Salary'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4qRbK-dgIs5"
      },
      "source": [
        "**Target Guided Ordinal Encoding** \n",
        "Ordering the labels according to the target\n",
        "Replace the labels by the joint probability of being 1 or 0 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-FsDS0nglLS"
      },
      "source": [
        "# Let's make simple and power full encoder\n",
        "df.apply(lambda x: len(x.unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4NZIxDBi0E1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgyQVugJh9zg"
      },
      "source": [
        "Mean Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-GWlGV8iypc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E84ucuLFlK40"
      },
      "source": [
        "# GroupBy is a powerful and versatile function in Python. It allows you \n",
        "#to split your data into separate groups to perform computations for better analysis\n",
        "\n",
        "\n",
        "# For emp_length column\n",
        "ordinal_labels=df.groupby(['emp_length'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['emp_length'])['Target_Satisfied'].mean().to_dict()\n",
        "df['emp_length']= df['emp_length'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "# For LOAN_AMT column\n",
        "df.LOAN_AMT.unique()\n",
        "df.groupby(['LOAN_AMT'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels=df.groupby(['LOAN_AMT'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['LOAN_AMT'])['Target_Satisfied'].mean().to_dict()\n",
        "df['LOAN_AMT']= df['LOAN_AMT'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# For Civil Service Title column\n",
        "df['Civil Service Title'].unique()\n",
        "ordinal_labels=df.groupby(['Civil Service Title'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['Civil Service Title'])['Target_Satisfied'].mean().to_dict()\n",
        "df['Civil Service Title']= df['Civil Service Title'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# For Division/Work Unit Title column\n",
        "df['Division/Work Unit'].unique()\n",
        "ordinal_labels=df.groupby(['Division/Work Unit'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['Division/Work Unit'])['Target_Satisfied'].mean().to_dict()\n",
        "df['Division/Work Unit']= df['Division/Work Unit'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "# For DevType column\n",
        "df['DevType'].unique()\n",
        "ordinal_labels=df.groupby(['DevType'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['DevType'])['Target_Satisfied'].mean().to_dict()\n",
        "df['DevType']= df['DevType'].map(mean_ordinal)\n",
        "\n",
        "# For earliest_cr_line column\n",
        "df['earliest_cr_line'].unique()\n",
        "ordinal_labels=df.groupby(['earliest_cr_line'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['earliest_cr_line'])['Target_Satisfied'].mean().to_dict()\n",
        "df['earliest_cr_line']= df['earliest_cr_line'].map(mean_ordinal)\n",
        "\n",
        "# For HopeFiveYears column\n",
        "df['HopeFiveYears'].unique()\n",
        "ordinal_labels=df.groupby(['HopeFiveYears'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['HopeFiveYears'])['Target_Satisfied'].mean().to_dict()\n",
        "df['HopeFiveYears']= df['HopeFiveYears'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# For Age column\n",
        "df['Age'].unique()\n",
        "ordinal_labels=df.groupby(['Age'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['Age'])['Target_Satisfied'].mean().to_dict()\n",
        "df['Age']= df['Age'].map(mean_ordinal)\n",
        "\n",
        "# For purpose column\n",
        "df['purpose'].unique()\n",
        "ordinal_labels=df.groupby(['purpose'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['purpose'])['Target_Satisfied'].mean().to_dict()\n",
        "df['purpose']= df['purpose'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "# For Title column\n",
        "df['title'].unique()\n",
        "ordinal_labels=df.groupby(['title'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['title'])['Target_Satisfied'].mean().to_dict()\n",
        "df['title']= df['title'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "# For last_pymnt_d column\n",
        "df['last_pymnt_d'].unique()\n",
        "ordinal_labels=df.groupby(['last_pymnt_d'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['last_pymnt_d'])['Target_Satisfied'].mean().to_dict()\n",
        "df['last_pymnt_d']= df['last_pymnt_d'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "# For last_credit_pull_d column\n",
        "df['last_credit_pull_d'].unique()\n",
        "ordinal_labels=df.groupby(['last_credit_pull_d'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['last_credit_pull_d'])['Target_Satisfied'].mean().to_dict()\n",
        "df['last_credit_pull_d']= df['last_credit_pull_d'].map(mean_ordinal)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SFlIna_73u_"
      },
      "source": [
        "**Binary Encoding**\n",
        "Binary encoding is a combination of Hash encoding and one-hot encoding. In this encoding scheme, the categorical feature is first converted into numerical using an ordinal encoder. Then the numbers are transformed in the binary number. After that binary value is split into different columns.Here We are using Binary Encoding for column that have less number of labels \n",
        "\n",
        "Binary encoding is a memory-efficient encoding scheme as it uses fewer features than one-hot encoding. Further, It reduces the curse of dimensionality for data with high cardinality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXTNm47Tmdub"
      },
      "source": [
        "encoder= ce.BinaryEncoder(cols=['UndergradMajor'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['UndergradMajor']) \n",
        "df2 = data_encoded.iloc[: , 1:]\n",
        "df_row = pd.concat([df, df2], axis=1)\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['CompanySize'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['CompanySize']) \n",
        "df4 = data_encoded.iloc[: , 1:]\n",
        "df_row = pd.concat([df_row, df4], axis=1)\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['YearsCodingProf'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['YearsCodingProf']) \n",
        "df5 = data_encoded.iloc[: , 1:]\n",
        "df_row = pd.concat([df_row, df5], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['JobSearchStatus'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['JobSearchStatus']) \n",
        "df6 = data_encoded.iloc[: , 1:]\n",
        "df_row = pd.concat([df_row, df6], axis=1)\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['LastNewJob'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['LastNewJob']) \n",
        "df7 = data_encoded.iloc[: , 1:]\n",
        "df_row = pd.concat([df_row, df7], axis=1)\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['AgreeDisagree1'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['AgreeDisagree1']) \n",
        "df8 = data_encoded.iloc[: , 1:]\n",
        "df_row = pd.concat([df_row, df8], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['term'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['term']) \n",
        "df9 = data_encoded.iloc[: , 1:]\n",
        "df_row = pd.concat([df_row, df9], axis=1)\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['loan_status'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['loan_status']) \n",
        "df10 = data_encoded.iloc[: , 1:]\n",
        "df_row = pd.concat([df_row, df10], axis=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PenUJqV-Yhw"
      },
      "source": [
        "# Dropping after Binary_encoding it\n",
        "df_cleaned = df_row.drop(['UndergradMajor', 'CompanySize', 'YearsCodingProf',\n",
        "             'JobSearchStatus', 'LastNewJob', 'AgreeDisagree1', 'term','loan_status'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxMBK6CYgQ4q"
      },
      "source": [
        "# df_cleaned.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQH_n1tDn9-X"
      },
      "source": [
        "# checking the correlation \n",
        "# df_cleaned.corr()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KBF1lpMeFw6"
      },
      "source": [
        "We Observed that many of the features have -ve correlation with Target continues variable. as per the domain expertise we selecting the columns\n",
        "But in the case Binary Target feature  have less number of negative correlation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3WYS61qorge"
      },
      "source": [
        "# Here the last_pymnt_amnt column have high -ve correlation \n",
        "# dropping the -ve correlate feature\n",
        "# iam not sure the last payment is important for the target.It isDomain Expert decision.\n",
        "# i trained with and with out dropping this features i observed this column have major role to up the error\n",
        "df_cleaned = df_cleaned.drop(['last_pymnt_amnt', 'dateAdded'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fctjY7aBh0me"
      },
      "source": [
        "df_cleaned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54oP2i6PGFUv"
      },
      "source": [
        "# checking the final that all features are ready to train\n",
        "df_cleaned.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYVZjZAZqzk7"
      },
      "source": [
        "## Dealing with Imbalanced Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf3jGUQUqxHe"
      },
      "source": [
        "\n",
        "# Splitting the Binary Target features\n",
        "target_satisfied = df_cleaned[df_cleaned['Target_Satisfied'] == 1]\n",
        "target_not_satisfied = df_cleaned[df_cleaned['Target_Satisfied'] == 0]\n",
        "\n",
        "#counting the value\n",
        "count_classes = pd.value_counts(df_cleaned['Target_Satisfied'], sort = True)\n",
        "# ploting bar graph\n",
        "count_classes.plot(kind = 'bar', rot=0)\n",
        "\n",
        "# labeling the graph\n",
        "plt.title(\"Transaction Class Distribution\")\n",
        "LABELS = ['Target Satisfied', 'Target_Not_Satisfied']\n",
        "plt.xticks(range(2), LABELS)\n",
        "\n",
        "plt.xlabel(\"Class\")\n",
        "\n",
        "plt.ylabel(\"Frequency\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDUp_n5zrCHb"
      },
      "source": [
        "**OverSampling**\n",
        "\n",
        "I observed that the data is not in-balanced so we are doing over sampling instead of under sampling because the records are not huge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJYw2nfLq_QO"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# shape Before over_sampling\n",
        "print('Before Over_Sampling', target_satisfied.shape, target_not_satisfied.shape)\n",
        "\n",
        "# Splitting Dependent And Independent Features\n",
        "y = df_cleaned['Target_Satisfied']\n",
        "X = df_cleaned.drop('Target_Satisfied', axis=1)\n",
        "\n",
        "\n",
        "\n",
        "os =  RandomOverSampler(ratio=0.5)\n",
        "X_cls, y_cls = os.fit_sample(X, y)\n",
        "\n",
        "# shape After over_sampling\n",
        "print('After Over_Sampling', X_cls.shape,y_cls.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWf3N7WyJHum"
      },
      "source": [
        "# # We need over sampled data for Regression Model\n",
        "# # we are converting numpy array to data frame\n",
        "\n",
        "# str = 'Target_Salary\tTarget_Satisfied\tLOAN_AMT\tBusiness Title\tCivil Service Title\tDivision/Work Unit\tFormalEducation\tDevType\tHopeFiveYears\tAge\tpurpose\ttitle\tdti\tearliest_cr_line\tlast_pymnt_d\tlast_credit_pull_d\temp_length\tUndergradMajor_1\tUndergradMajor_2\tUndergradMajor_3\tUndergradMajor_4\tCompanySize_1\tCompanySize_2\tCompanySize_3\tYearsCodingProf_1\tYearsCodingProf_2\tYearsCodingProf_3\tYearsCodingProf_4\tJobSearchStatus_1\tJobSearchStatus_2\tLastNewJob_1\tLastNewJob_2\tLastNewJob_3\tAgreeDisagree1_1\tAgreeDisagree1_2\tAgreeDisagree1_3\tterm_1\tloan_status_1\tloan_status_2\tloan_status_3'\n",
        "# # splitting the so\n",
        "# columns_list = list(str.split())\n",
        "# columns_list\n",
        "# # df = pd.DataFrame(data = X_aftr_smplng, \n",
        "# #                   index = index_values, \n",
        "# #                   columns = columns_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QG2Jh4tjOeI"
      },
      "source": [
        "# Splitting Dependent And Independent Features\n",
        "y = df_cleaned['Target_Satisfied']\n",
        "X = df_cleaned.drop('Target_Salary', axis=1)\n",
        "\n",
        "\n",
        "\n",
        "os =  RandomOverSampler(ratio=0.5)\n",
        "X_reg, y_reg = os.fit_sample(X, y)\n",
        "\n",
        "# shape After over_sampling\n",
        "print('After Over_Sampling', X_reg.shape,y_reg.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpzrgBMZrrFO"
      },
      "source": [
        "# Counter will help to count labels\n",
        "# riginal dataset shape Counter\n",
        "# Resampled dataset shape Counter\n",
        "from collections import Counter\n",
        "print('Original dataset shape {}'.format(Counter(y)))\n",
        "print('Resampled dataset shape {}'.format(Counter(y_train_res)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_VJfLBVK9zj"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHWVe7MI56Ir"
      },
      "source": [
        "# Randomly splitting \n",
        "from sklearn.model_selection import train_test_split\n",
        "X = X_aftr_smplng\n",
        "y = y_aftr_smplng\n",
        "\n",
        "\n",
        "# Randomly splitting train-test data (80,20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_cls, y_cls, test_size = 0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UwalFroLUZ8"
      },
      "source": [
        "# Building Classification Model To Predict Satisfied/Not-Satisfied\n",
        "\n",
        "Here I used RandomForestClassier As Per The Requirements  For Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGCJdN51PaKM"
      },
      "source": [
        "# # Number of trees in random forest\n",
        "# n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
        "# # Number of features to consider at every split\n",
        "# max_features = ['auto', 'sqrt']\n",
        "# # Maximum number of levels in tree\n",
        "# max_depth = [2,4]\n",
        "# # Minimum number of samples required to split a node\n",
        "# min_samples_split = [2, 5]\n",
        "# # Minimum number of samples required at each leaf node\n",
        "# min_samples_leaf = [1, 2]\n",
        "# # Method of selecting samples for training each tree\n",
        "# bootstrap = [True, False]\n",
        "\n",
        "# # Create the param grid\n",
        "# param_grid = {'n_estimators': n_estimators,\n",
        "#                'max_features': max_features,\n",
        "#                'max_depth': max_depth,\n",
        "#                'min_samples_split': min_samples_split,\n",
        "#                'min_samples_leaf': min_samples_leaf,\n",
        "#                'bootstrap': bootstrap}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgrVgnriPeQm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHCAA_DaGKpT"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykOsqd_pPioP"
      },
      "source": [
        "# clf_Model = RandomForestClassifier()\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# clf_Grid = GridSearchCV(estimator = clf_Model, param_grid = param_grid, cv = 3, verbose=2, n_jobs = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUA9paCsPpx7"
      },
      "source": [
        "# clf_Grid.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T3ufaOxGZEa"
      },
      "source": [
        "# Default pretrained Random Forest giving better accuracy \n",
        "# Importing the RFC from sklearn\n",
        "clf = RandomForestClassifier(n_estimators = 100)\n",
        "# Fitting the training  featrues and target\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D2YdV-YL6_I"
      },
      "source": [
        "# Model Evaluating\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdw1jf6A60en"
      },
      "source": [
        "**WEIGHTED F1 SCORE: 0.9376344086021505\n",
        "Screenshot:(https://github.com/shamil-k/Salary-And-Satisfaction-Prediction-Using_RandomForest)\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_JLF89iHx9h"
      },
      "source": [
        "# performing predictions on the test dataset\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# metrics are used to find accuracy or error\n",
        "from sklearn import metrics  \n",
        "print()\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "print('WEIGHTED F1 SCORE :', f1_score(y_test,y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yXucXUuPyD4"
      },
      "source": [
        "Model Deployment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3TsZveIN6-i"
      },
      "source": [
        "\n",
        "# import joblib\n",
        "# # saving the dataframe\n",
        "# df_cleaned.to_csv('After_Preprocessing.csv')\n",
        "\n",
        "# # save the model to disk here iam saving my drive. \n",
        "# # also shared the model and data after preprocessing on git\n",
        "# model = clf\n",
        "# filename = '/content/drive/MyDrive/New folder/CLF_Model.pkl'\n",
        "# joblib.dump(model, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gfH4kmNBvtg"
      },
      "source": [
        "# Load the model from the file\n",
        "# RFC_from_joblib = joblib.load('/content/drive/MyDrive/New folder/CLF_Model.pkl') \n",
        "  \n",
        "# # Use the loaded model to make predictions\n",
        "# y_pred = RFC_from_joblib.predict(X_test)\n",
        "  \n",
        "# # using metrics module for accuracy calculation\n",
        "# print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred))\n",
        "# print('', f1_score(y_test,y_pred))\n",
        "# print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_mvHXFZDIgY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRMjJcu-5efK"
      },
      "source": [
        "# Building a Regression Model To Predict Target Salary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_f5bIrzkb_m"
      },
      "source": [
        "# Randomly splitting train-test data (80,20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size = 0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-AUvYt5Phya"
      },
      "source": [
        "# importing FRF From sklearn\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Fitting the Model for train\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP2vgBYz2pt4"
      },
      "source": [
        "# Accuracy\n",
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "from math import sqrt\n",
        "# Evaluating the test test data\n",
        "y_pred = model.predict(X_test)\n",
        "# getting error score\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('Mean_Squered_Error', mse)\n",
        "# R^2 Value\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('R^2 SCORE:',r2)\n",
        "\n",
        "# Finding the Root Mean Squre Error\n",
        "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "print('ROOT MEAN SQUERE ERROR: ',rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL2vn5oj6_c0"
      },
      "source": [
        ""
      ]
    }
  ]
}