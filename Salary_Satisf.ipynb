{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Salary_Satisf.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1B87DEmziga6vSyGJgU7HKEgiukDIrueD",
      "authorship_tag": "ABX9TyOVbquPGH7b5V/tq1VMGMU3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shamil-k/Salary-And-Satisfaction-Prediction-Using_RandomForest/blob/main/Salary_Satisf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwLhqBBYEGEq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ56wwblSHg3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcuoxnRyEOpm"
      },
      "source": [
        "#  Business Understanding \n",
        "\n",
        "You have to clean this data, prepare it for ML models, build more features and ultimately build a random forest (scikit).\n",
        "Split the dataset into 80:20, train:test. \n",
        "Print the Weighted F1 on test dataset (target_satisfaction) and RMSE (Target_salary)\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhRxE8okT_dT"
      },
      "source": [
        "# some of my references:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfR7-p0BRbC4"
      },
      "source": [
        "\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
        "\n",
        "https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf\n",
        "\n",
        "https://github.com/shamil-k/Feature-Engineer-All-You-need\n",
        "\n",
        "\n",
        "https://towardsdatascience.com/getting-deeper-into-categorical-encodings-for-machine-learning-2312acd347c8\n",
        "\n",
        "https://github.com/shamil-k/Feature-Engineer-All-You-need\n",
        "                https://heartbeat.fritz.ai/\n",
        "              \n",
        "\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/\n",
        "               \n",
        "               \n",
        "https://contrib.scikit-learn.org/category_encoders/hashing.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1gXCV33UJQf"
      },
      "source": [
        "# Data Mining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buwdtfXvntXR"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "!pip install category_encoders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ehZPo6zDjsk"
      },
      "source": [
        "\n",
        "**Reading The Data Set Using Pandas.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLGlG5ktb3aR"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/New folder/salary,satisfaction - salary,satisfaction.csv', parse_dates=True, dayfirst=True, index_col=0)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UgLSnKND3-0"
      },
      "source": [
        "# Data Cleaning/Preprocessing  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aztw2UsLb70N"
      },
      "source": [
        "\n",
        "df.isnull().sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu4ug79_VSdf"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2D0muTSXHCb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfyWJWiFcNYS"
      },
      "source": [
        "\n",
        "df['FormalEducation'] = df['FormalEducation'].fillna('Not-Specified')\n",
        "df['UndergradMajor'] = df['UndergradMajor'].fillna('Not-Specified')\n",
        "df['DevType'] = df['DevType'].fillna('Not-Specified')\n",
        "df['HopeFiveYears'] = df['HopeFiveYears'].fillna('Not-Specified')\n",
        "df['YearsCoding'] = df['YearsCoding'].fillna('0-2')\n",
        "df['YearsCodingProf'] = df['YearsCodingProf'].fillna('0-2')\n",
        "df['JobSearchStatus'] = df['JobSearchStatus'].fillna('Not-Specified')\n",
        "df['LastNewJob'] = df['LastNewJob'].fillna('Not-Specified')\n",
        "df['EducationTypes'] = df['EducationTypes'].fillna('Not-Specified')\n",
        "df['AgreeDisagree1'] = df['AgreeDisagree1'].fillna('Not-Specified')\n",
        "df['Age'] = df['Age'].fillna('Not-Specified')\n",
        "df['emp_length'] = df['emp_length'].fillna('< 1 year')\n",
        "df['Age'] = df['Age'].fillna('Not-Specified')\n",
        "df['title'] = df['title'].fillna('Not-Specified')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCZGz84fXoaC"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdOQX3fm0qIS"
      },
      "source": [
        "chars_to_remove = ['years', 'or more years', 'years old', '$', '<']\n",
        "regular_expression = '[' + re.escape (''. join (chars_to_remove)) + ']'\n",
        "df['YearsCodingProf'] = df['YearsCodingProf'].str.replace(regular_expression, '', regex=True) \n",
        "df['title'] = df['title'].str.replace(regular_expression, '', regex = True)\n",
        "df['LOAN_AMT'] = df['LOAN_AMT'].str.replace(regular_expression, '', regex = True)\n",
        "df['emp_length'] = df['emp_length'].str.replace(regular_expression, '', regex = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VZvEtcu00JC"
      },
      "source": [
        "df  = df.reset_index()\n",
        "df = df.drop(['id', 'Time', 'EducationTypes',  'latitude', 'longitude', 'YearsCoding' ], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WMrV5wo2Aep"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_dqEZOL2HP_"
      },
      "source": [
        "chars_to_remove = ['years', 'or more years', 'years old', '$']\n",
        "regular_expression = '[' + re.escape (''. join (chars_to_remove)) + ']'\n",
        "df['YearsCodingProf'] = df['YearsCodingProf'].str.replace(regular_expression, '', regex=True) \n",
        "df['title'] = df['title'].str.replace(regular_expression, '', regex = True)\n",
        "df['LOAN_AMT'] = df['LOAN_AMT'].str.replace(regular_expression, '', regex = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrY5xd0O3ln7"
      },
      "source": [
        "df = df.drop('dateAdded', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_z9PXmHFrdj"
      },
      "source": [
        "## Encoding Categorical Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngtw50F26i0D"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import category_encoders as ce"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZFrfDH03WRv"
      },
      "source": [
        "df.apply(lambda x: len(x.unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KTzsO-bbSmI"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDHmFZptf5uV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnqRGke18w7j"
      },
      "source": [
        "encoder = ce.TargetEncoder(cols=['Business Title'])\n",
        "encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzNzZaUOBGE7"
      },
      "source": [
        "df['Business Title'] = encoder.fit_transform(df['Business Title'],df['Target_Salary'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96l63gNJCZtT"
      },
      "source": [
        "encoder = ce.TargetEncoder(cols=['FormalEducation'])\n",
        "encoder\n",
        "df['FormalEducation'] = encoder.fit_transform(df['FormalEducation'],df['Target_Salary'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4qRbK-dgIs5"
      },
      "source": [
        "**Target Guided Ordinal Encoding** \n",
        "Ordering the labels according to the target\n",
        "Replace the labels by the joint probability of being 1 or 0 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-FsDS0nglLS"
      },
      "source": [
        "df.apply(lambda x: len(x.unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4NZIxDBi0E1"
      },
      "source": [
        "df.emp_length.unique()\n",
        "\n",
        "df.groupby(['emp_length'])['Target_Satisfied'].mean().sort_values().index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbPTDLYaj9MP"
      },
      "source": [
        "ordinal_labels=df.groupby(['emp_length'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goZ-Z0FdkFXg"
      },
      "source": [
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "ordinal_labels2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD8axK_skMe1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgyQVugJh9zg"
      },
      "source": [
        "Mean Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abs1FP3MkSdO"
      },
      "source": [
        "mean_ordinal=df.groupby(['emp_length'])['Target_Satisfied'].mean().to_dict()\n",
        "mean_ordinal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-GWlGV8iypc"
      },
      "source": [
        "df['emp_length']=df['emp_length'].map(mean_ordinal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E84ucuLFlK40"
      },
      "source": [
        "# For LOAN_AMT column\n",
        "df.LOAN_AMT.unique()\n",
        "df.groupby(['LOAN_AMT'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels=df.groupby(['LOAN_AMT'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['LOAN_AMT'])['Target_Satisfied'].mean().to_dict()\n",
        "df['LOAN_AMT']= df['LOAN_AMT'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# For Civil Service Title column\n",
        "df['Civil Service Title'].unique()\n",
        "ordinal_labels=df.groupby(['Civil Service Title'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['Civil Service Title'])['Target_Satisfied'].mean().to_dict()\n",
        "df['Civil Service Title']= df['Civil Service Title'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# For Division/Work Unit Title column\n",
        "df['Division/Work Unit'].unique()\n",
        "ordinal_labels=df.groupby(['Division/Work Unit'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['Division/Work Unit'])['Target_Satisfied'].mean().to_dict()\n",
        "df['Division/Work Unit']= df['Division/Work Unit'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "# For DevType column\n",
        "df['DevType'].unique()\n",
        "ordinal_labels=df.groupby(['DevType'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['DevType'])['Target_Satisfied'].mean().to_dict()\n",
        "df['DevType']= df['DevType'].map(mean_ordinal)\n",
        "\n",
        "# For earliest_cr_line column\n",
        "df['earliest_cr_line'].unique()\n",
        "ordinal_labels=df.groupby(['earliest_cr_line'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['earliest_cr_line'])['Target_Satisfied'].mean().to_dict()\n",
        "df['earliest_cr_line']= df['earliest_cr_line'].map(mean_ordinal)\n",
        "\n",
        "# For HopeFiveYears column\n",
        "df['HopeFiveYears'].unique()\n",
        "ordinal_labels=df.groupby(['HopeFiveYears'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['HopeFiveYears'])['Target_Satisfied'].mean().to_dict()\n",
        "df['HopeFiveYears']= df['HopeFiveYears'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# For Age column\n",
        "df['Age'].unique()\n",
        "ordinal_labels=df.groupby(['Age'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['Age'])['Target_Satisfied'].mean().to_dict()\n",
        "df['Age']= df['Age'].map(mean_ordinal)\n",
        "\n",
        "# For purpose column\n",
        "df['purpose'].unique()\n",
        "ordinal_labels=df.groupby(['purpose'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['purpose'])['Target_Satisfied'].mean().to_dict()\n",
        "df['purpose']= df['purpose'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "# For Title column\n",
        "df['title'].unique()\n",
        "ordinal_labels=df.groupby(['title'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['title'])['Target_Satisfied'].mean().to_dict()\n",
        "df['title']= df['title'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "# For last_pymnt_d column\n",
        "df['last_pymnt_d'].unique()\n",
        "ordinal_labels=df.groupby(['last_pymnt_d'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['last_pymnt_d'])['Target_Satisfied'].mean().to_dict()\n",
        "df['last_pymnt_d']= df['last_pymnt_d'].map(mean_ordinal)\n",
        "\n",
        "\n",
        "\n",
        "# For last_credit_pull_d column\n",
        "df['last_credit_pull_d'].unique()\n",
        "ordinal_labels=df.groupby(['last_credit_pull_d'])['Target_Satisfied'].mean().sort_values().index\n",
        "ordinal_labels2={k:i for i,k in enumerate(ordinal_labels,0)}\n",
        "mean_ordinal=df.groupby(['last_credit_pull_d'])['Target_Satisfied'].mean().to_dict()\n",
        "df['last_credit_pull_d']= df['last_credit_pull_d'].map(mean_ordinal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SFlIna_73u_"
      },
      "source": [
        "**Binary Encoding**\n",
        "Binary encoding is a combination of Hash encoding and one-hot encoding. In this encoding scheme, the categorical feature is first converted into numerical using an ordinal encoder. Then the numbers are transformed in the binary number. After that binary value is split into different columns.Here We are using Binary Encoding for column that have less number of labels \n",
        "\n",
        "Binary encoding is a memory-efficient encoding scheme as it uses fewer features than one-hot encoding. Further, It reduces the curse of dimensionality for data with high cardinality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXTNm47Tmdub"
      },
      "source": [
        "encoder= ce.BinaryEncoder(cols=['UndergradMajor'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['UndergradMajor']) \n",
        "df2 = data_encoded\n",
        "df_row = pd.concat([df, df2], axis=1)\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['CompanySize'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['CompanySize']) \n",
        "df4 = data_encoded\n",
        "df_row = pd.concat([df_row, df4], axis=1)\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['YearsCodingProf'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['YearsCodingProf']) \n",
        "df5 = data_encoded\n",
        "df_row = pd.concat([df_row, df5], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['YearsCodingProf'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['YearsCodingProf']) \n",
        "df5 = data_encoded\n",
        "df_row = pd.concat([df_row, df5], axis=1)\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['JobSearchStatus'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['JobSearchStatus']) \n",
        "df6 = data_encoded\n",
        "df_row = pd.concat([df_row, df6], axis=1)\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['LastNewJob'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['LastNewJob']) \n",
        "df7 = data_encoded\n",
        "df_row = pd.concat([df_row, df7], axis=1)\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['AgreeDisagree1'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['AgreeDisagree1']) \n",
        "df8 = data_encoded\n",
        "df_row = pd.concat([df_row, df8], axis=1)\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['AgreeDisagree1'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['AgreeDisagree1']) \n",
        "df8 = data_encoded\n",
        "df_row = pd.concat([df_row, df8], axis=1)\n",
        "\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['term'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['term']) \n",
        "df9 = data_encoded\n",
        "df_row = pd.concat([df_row, df9], axis=1)\n",
        "\n",
        "encoder= ce.BinaryEncoder(cols=['loan_status'],return_df=True)\n",
        "#Fit and Transform Data \n",
        "data_encoded=encoder.fit_transform(df['loan_status']) \n",
        "df10 = data_encoded\n",
        "df_row = pd.concat([df_row, df10], axis=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PenUJqV-Yhw"
      },
      "source": [
        "df_cleaned = df_row.drop(['UndergradMajor', 'CompanySize', 'YearsCodingProf',\n",
        "             'JobSearchStatus', 'LastNewJob', 'AgreeDisagree1', 'term','loan_status'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQH_n1tDn9-X"
      },
      "source": [
        "# checking the correlation \n",
        "\n",
        "\n",
        "df.corr()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3WYS61qorge"
      },
      "source": [
        "# Here the last_pymnt_amnt column have high -ve correlation \n",
        "# dropping the -ve correlate feature\n",
        "df_cleaned.drop('last_pymnt_amnt', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54oP2i6PGFUv"
      },
      "source": [
        "df_cleaned.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYVZjZAZqzk7"
      },
      "source": [
        "## Dealing with Imbalanced Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf3jGUQUqxHe"
      },
      "source": [
        "target_satisfied = df[df['Target_Satisfied'] == 1]\n",
        "target_not_satisfied = df[df['Target_Satisfied'] == 0]\n",
        "\n",
        "count_classes = pd.value_counts(df['Target_Satisfied'], sort = True)\n",
        "\n",
        "count_classes.plot(kind = 'bar', rot=0)\n",
        "\n",
        "plt.title(\"Transaction Class Distribution\")\n",
        "LABELS = ['Target Satisfied', 'Target_Not_Satisfied']\n",
        "plt.xticks(range(2), LABELS)\n",
        "\n",
        "plt.xlabel(\"Class\")\n",
        "\n",
        "plt.ylabel(\"Frequency\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDUp_n5zrCHb"
      },
      "source": [
        "**OverSampling**\n",
        "\n",
        "I observed that the data is not in-balanced so we are doing over sampling instead of under sampling because the records are not huge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJYw2nfLq_QO"
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# shape Before over_sampling\n",
        "print('Before Over_Sampling', target_satisfied.shape, target_not_satisfied.shape)\n",
        "\n",
        "# Splitting Dependent And Independent Features\n",
        "X = df_cleaned.drop('Target_Satisfied', axis=1)\n",
        "y = df_cleaned['Target_Satisfied']\n",
        "\n",
        "\n",
        "os =  RandomOverSampler(ratio=0.5)\n",
        "X_train_res, y_train_res = os.fit_sample(X, y)\n",
        "\n",
        "# shape After over_sampling\n",
        "print('After Over_Sampling', X_train_res.shape,y_train_res.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWf3N7WyJHum"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpzrgBMZrrFO"
      },
      "source": [
        "# Counter will help to count labels\n",
        "# riginal dataset shape Counter\n",
        "# Resampled dataset shape Counter\n",
        "from collections import Counter\n",
        "print('Original dataset shape {}'.format(Counter(y)))\n",
        "print('Resampled dataset shape {}'.format(Counter(y_train_res)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_VJfLBVK9zj"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHWVe7MI56Ir"
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = X_train_res\n",
        "y = y_train_res\n",
        "\n",
        "\n",
        "# Randomly splitting train-test data (80,20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UwalFroLUZ8"
      },
      "source": [
        "# Building Classification Model To Predict Satisfied/Not-Satisfied\n",
        "\n",
        "Here I used RandomForestClassier As Per The Requirements  For Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHCAA_DaGKpT"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T3ufaOxGZEa"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators = 100) \n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D2YdV-YL6_I"
      },
      "source": [
        "# Model Evaluating\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdw1jf6A60en"
      },
      "source": [
        "**WEIGHTED F1 SCORE: 0.926605504587156**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_JLF89iHx9h"
      },
      "source": [
        "# performing predictions on the test dataset\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# metrics are used to find accuracy or error\n",
        "from sklearn import metrics  \n",
        "print()\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "print('WEIGHTED F1 SCORE :', f1_score(y_test,y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yXucXUuPyD4"
      },
      "source": [
        "Model Deployment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3TsZveIN6-i"
      },
      "source": [
        "\n",
        "import joblib\n",
        "# saving the dataframe\n",
        "df_cleaned.to_csv('After_Preprocessing.csv')\n",
        "\n",
        "# save the model to disk here iam saving my drive. \n",
        "# also shared the model and data after preprocessing on git\n",
        "model = clf\n",
        "filename = '/content/drive/MyDrive/New folder/CLF_Model.pkl'\n",
        "joblib.dump(model, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gfH4kmNBvtg"
      },
      "source": [
        "# Load the model from the file\n",
        "# RFC_from_joblib = joblib.load('/content/drive/MyDrive/New folder/CLF_Model.pkl') \n",
        "  \n",
        "# # Use the loaded model to make predictions\n",
        "# y_pred = RFC_from_joblib.predict(X_test)\n",
        "  \n",
        "# # using metrics module for accuracy calculation\n",
        "# print(\"ACCURACY OF THE MODEL: \", metrics.accuracy_score(y_test, y_pred))\n",
        "# print('', f1_score(y_test,y_pred))\n",
        "# print(metrics.confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_mvHXFZDIgY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRMjJcu-5efK"
      },
      "source": [
        "# Building a Regression Model To Predict Target Salary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-AUvYt5Phya"
      },
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP2vgBYz2pt4"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "from math import sqrt\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('Mean_Squered_Error', mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print('R^2 SCORE:',r2)\n",
        "\n",
        "\n",
        "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "print('ROOT MEAN SQUERE ERROR: ',rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL2vn5oj6_c0"
      },
      "source": [
        "**ROOT MEAN SQUERE ERROR:0.2225828457207991**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2NUkoE03MoK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}